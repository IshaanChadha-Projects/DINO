{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd74515",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT LIBRARIES ###\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize\n",
    "from torchvision import models\n",
    "from torchvision.models import efficientnet_b0\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7157ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPARATION FUNCTION ###\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(dataset_directory):\n",
    "    # List and sort the files in the dataset directory\n",
    "    sorted_file_list = os.listdir(dataset_directory)\n",
    "    sorted_file_list.sort()\n",
    "    # Skip the first item if it's a system file\n",
    "    sorted_file_list = sorted_file_list[1:]\n",
    "\n",
    "    # Initialize an empty DataFrame for image paths and their types\n",
    "    image_data = pd.DataFrame(columns=['Image path', 'Type'])\n",
    "\n",
    "    # Loop through each category folder\n",
    "    for category in sorted_file_list:\n",
    "        folder_path = os.path.join(dataset_directory, category)\n",
    "        # Filter for image files only\n",
    "        image_files = [file for file in os.listdir(folder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "                       and not file.startswith('.')]\n",
    "\n",
    "        # Add image path and category to the DataFrame\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image_data.loc[len(image_data)] = [image_path, category]\n",
    "\n",
    "    # Map each category to a unique label\n",
    "    type_to_label = {type_name: index for index, type_name in enumerate(image_data['Type'].unique())}\n",
    "    image_data['Label'] = image_data['Type'].apply(lambda x: type_to_label[x])\n",
    "\n",
    "    return image_data, type_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET AND DATALOADER SETUP FUNCTION###\n",
    "\n",
    "\n",
    "# Custom dataset class for loading and transforming images\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.dataframe.iloc[idx]['Image path']\n",
    "        label = self.dataframe.iloc[idx]['Label']\n",
    "        image = Image.open(image_path).convert('RGB')  # Convert images to RGB format\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "def setup_data_loaders(image_data, data_transforms, batch_size=4):\n",
    "    # Initialize the custom dataset with the provided data and transformations\n",
    "    custom_dataset = CustomDataset(dataframe=image_data, transform=data_transforms)\n",
    "\n",
    "    # Split the dataset indices into training, validation, and testing sets\n",
    "    train_val_indices, test_indices = train_test_split(image_data.index.tolist(),\n",
    "                                                       test_size=0.2, stratify=image_data['Label'].values)\n",
    "    train_indices, val_indices = train_test_split(train_val_indices,\n",
    "                                            test_size=0.25, stratify=image_data['Label'].values[train_val_indices])\n",
    "\n",
    "    # Create subsets for training, validation, and testing\n",
    "    train_dataset = Subset(custom_dataset, train_indices)\n",
    "    val_dataset = Subset(custom_dataset, val_indices)\n",
    "    test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "    # Initialize DataLoader for each set\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISPLAY IMAGES FUNCTION ###\n",
    "\n",
    "\n",
    "def show_images(images, labels):\n",
    "    # Determine the number of images\n",
    "    num_images = len(images)\n",
    "    # Set up a grid of plots\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(12, 4))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Convert tensor to numpy and normalize the pixel values to [0, 1]\n",
    "        image = np.clip(images[i].permute(1, 2, 0).numpy(), 0, 1)\n",
    "        label = labels[i].item()\n",
    "        # Display the image and its label\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL SETUP FUNCTION ###\n",
    "\n",
    "\n",
    "def setup_model(num_classes, device):\n",
    "    # Load a pre-trained ResNet18 model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # Freeze all model parameters to prevent updating during training\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final layer to match the number of classes\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a127170",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING LOOP WITH EARLY STOPPING FUNCTION ###\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, patience=5):\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss for early stopping\n",
    "    train_loss_values, val_loss_values = [], []  # Lists to track loss values\n",
    "    epoch = 0\n",
    "\n",
    "    while True:\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_train_loss += loss.item()  # Accumulate loss\n",
    "        train_loss_values.append(total_train_loss / len(train_loader))  # Compute average training loss\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():  # No gradient computation in validation phase\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  # Move data to device\n",
    "                val_outputs = model(val_inputs)  # Forward pass\n",
    "                total_val_loss += criterion(val_outputs, val_labels).item()  # Accumulate loss\n",
    "        total_val_loss /= len(val_loader)  # Compute average validation loss\n",
    "        val_loss_values.append(total_val_loss)  # Track validation loss\n",
    "        \n",
    "        # Clear previous output\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Plot training and validation loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss_values, marker='o', linestyle='-', color='b', label='Train Loss')\n",
    "        plt.plot(val_loss_values, marker='o', linestyle='-', color='r', label='Val Loss')\n",
    "        plt.title('Train vs Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"-\")\n",
    "        plt.xticks(range(epoch + 1))\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {loss_values[-1]}, Validation Loss: {val_loss_values[-1]}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss  # Update best validation loss\n",
    "            best_model_state = model.state_dict()  # Save best model state\n",
    "            epochs_no_improve = 0  # Reset counter for epochs without improvement\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment counter\n",
    "            if epochs_no_improve == patience:  # Check if patience limit is reached\n",
    "                print(f\"Early stopping at epoch {epoch+1}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "        \n",
    "        epoch += 1  # Increment epoch counter\n",
    "\n",
    "    return model, best_model_state, train_loss_values, val_loss_values  # Return model, best state, and loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e632434",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST MODEL FUNCTION ###\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device, class_idx_to_label):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store all predictions and labels\n",
    "    all_predictions, all_labels = [], []\n",
    "    \n",
    "    # No gradient computation needed\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes\n",
    "            \n",
    "            # Append predictions and labels for later evaluation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = 100 * np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=[class_idx_to_label[i] \n",
    "                                                                        for i in range(len(class_idx_to_label))]))\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[class_idx_to_label[i]\n",
    "                                for i in range(len(class_idx_to_label))], yticklabels=[class_idx_to_label[i]\n",
    "                                                                       for i in range(len(class_idx_to_label))])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6efd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE THE DATA ###\n",
    "\n",
    "dataset_directory = './data/archive'  # Ensure this is the correct path to your dataset\n",
    "image_data, type_to_label = prepare_data(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET UP DATALOADERS ###\n",
    "\n",
    "# Data augmentation and normalization for training and validation\n",
    "data_transforms = transforms.Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize DataLoaders for training, validation, and testing\n",
    "batch_size = 4  # You can adjust this according to your system's capabilities\n",
    "train_loader, val_loader, test_loader = setup_data_loaders(image_data, data_transforms, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISPLAY A BATCH OF IMAGES ###\n",
    "\n",
    "# Display a batch of images from the training set\n",
    "for _ in range(2):\n",
    "    for batch_images, batch_labels in train_loader:\n",
    "        show_images(batch_images, batch_labels)\n",
    "        break  # Only display one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3198a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP THE MODEL ###\n",
    "\n",
    "# Check for device availability (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model, criterion (loss function), and optimizer\n",
    "num_classes = len(type_to_label)  # The number of classes in your dataset\n",
    "model, criterion, optimizer = setup_model(num_classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN THE MODEL ###\n",
    "\n",
    "# Train the model (this might take a while depending on your dataset and system)\n",
    "model, best_model_state, train_loss_values, val_loss_values = \\\n",
    "    train_model(model, criterion, optimizer, train_loader, val_loader, device, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST THE MODEL ###\n",
    "\n",
    "# Loads the best state of the model\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Reverse the `type_to_label` dictionary to get class indices mapped back to their labels\n",
    "class_idx_to_label = {v: k for k, v in type_to_label.items()}\n",
    "\n",
    "# Call the `test_model` function\n",
    "test_model(model, test_loader, device, class_idx_to_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
